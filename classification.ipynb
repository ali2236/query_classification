{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "285487e6",
   "metadata": {},
   "source": [
    "# IR Mini Project 2\n",
    "Ali Ghanbari - 970216657"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7149791a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "824ed107",
   "metadata": {},
   "source": [
    "1. Download dataset if necessery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "cb91f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'train.txt' already there; not retrieving.\n",
      "File 'test.txt' already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!wget https: // cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label -O train.txt -nc\n",
    "!wget https: // cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label -O test.txt -nc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60331ff8",
   "metadata": {},
   "source": [
    "2. Define text preprocessor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "dcebe9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aligator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aligator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aligator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "75888408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "english_stopwords.remove('who')\n",
    "english_stopwords.remove('where')\n",
    "english_stopwords.remove('what')\n",
    "english_stopwords.remove('how')\n",
    "\n",
    "\n",
    "def preprocess_text(txt: str) -> str:\n",
    "    tokens = word_tokenize(txt)\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    tokens = [w for w in tokens if w not in english_stopwords]\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    tokens = [lemma.lemmatize(w, pos = \"v\") for w in tokens]\n",
    "    tokens = [lemma.lemmatize(w, pos = \"n\") for w in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba75ddad",
   "metadata": {},
   "source": [
    "3. Define data parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "cc0fa39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data(parent class, child class, query, vector)\n",
    "from typing import Iterable\n",
    "class QueryRow:\n",
    "    def __init__(self, parent_class : str, child_class : str, query: str, vector) -> None:\n",
    "        self.parent_class = parent_class\n",
    "        self.child_class = child_class\n",
    "        self.query = query\n",
    "        self.vector = vector\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.parent_class}:{self.child_class} {self.query} - {self.vector}'\n",
    "        \n",
    "\n",
    "def parse_line(line) -> QueryRow:\n",
    "    spline = line.split()\n",
    "    labels = spline[0]\n",
    "    text = spline[1:-1]\n",
    "    splbl = labels.split(':')\n",
    "    parent_class = splbl[0]\n",
    "    child_class = splbl[1]\n",
    "    query = preprocess_text(' '.join(text))\n",
    "    return QueryRow(parent_class, child_class, query, [])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07df2fdf",
   "metadata": {},
   "source": [
    "4. Load and preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "2c3691d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "with open('train.txt') as f:\n",
    "    train_data = [parse_line(line) for line in f.readlines() if line]\n",
    "with open('test.txt') as f:\n",
    "    test_data = [parse_line(line) for line in f.readlines() if line]\n",
    "\n",
    "# print empty queries\n",
    "empty_queries = list(filter(lambda q: not q.query,train_data))\n",
    "empty_queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59fede7a",
   "metadata": {},
   "source": [
    "5. Vectorieze queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "132d9f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5952,)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(sublinear_tf=False, use_idf=True, norm='l2')\n",
    "train_queries = tfidf_vec.fit([q.query for q in train_data])\n",
    "\n",
    "all_data = train_data + test_data\n",
    "for q in all_data:\n",
    "    q.vector = tfidf_vec.transform([q.query])[0]\n",
    "\n",
    "np.shape(all_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "320dd827",
   "metadata": {},
   "source": [
    "---\n",
    "# Single Level Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c4357c9",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "9c6bcc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6334)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def make_2d(arr):\n",
    "    nsamples, nx, ny = arr.shape\n",
    "    return arr.reshape((nsamples,nx*ny))\n",
    "\n",
    "train_x = make_2d(np.array([q.vector.toarray() for q in train_data]))\n",
    "train_y = [q.parent_class for q in train_data]\n",
    "test_x = make_2d(np.array([q.vector.toarray() for q in test_data]))\n",
    "test_y = [q.parent_class for q in test_data]\n",
    "\n",
    "test_x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fcd3f64",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c890887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def class_accuracy_score(true, pred, average=None):\n",
    "    class_preds_tp = {}\n",
    "    class_preds_fn = {}\n",
    "    for i in range(len(true)):\n",
    "        c = true[i]\n",
    "        p = pred[i]\n",
    "        if c == p:\n",
    "            class_preds_tp[c] = class_preds_tp.get(c, 0) + 1\n",
    "        else:\n",
    "            class_preds_fn[c] = class_preds_fn.get(c, 0) + 1\n",
    "    all_classes = set(class_preds_fn.keys()).union(class_preds_tp.keys())\n",
    "    class_accuracy = {}\n",
    "    for cls in all_classes:\n",
    "        tp = class_preds_tp.get(cls, 0)\n",
    "        fn = class_preds_fn.get(cls, 0)\n",
    "        class_accuracy[cls] = tp / (tp + fn)\n",
    "    if average == \"micro\":\n",
    "        return np.average(list(class_accuracy.values()))\n",
    "    elif average == \"macro\":\n",
    "        return (np.sum(list(class_preds_tp.values()))) / (np.sum(list(class_preds_tp.values())) + np.sum(list(class_preds_fn.values())))\n",
    "    return class_accuracy\n",
    "\n",
    "\n",
    "benchmark_results = {}\n",
    "# benchmark(name, accuracy macro, accuracy micro, precision macro, precision micro, recall macro, recall micro, time train, time test)\n",
    "def benchmark_single_class(classifier, name: str):\n",
    "    train_start = time.process_time()\n",
    "    classifier.fit(train_x, train_y)\n",
    "    train_end = time.process_time()\n",
    "    train_time = train_end - train_start\n",
    "    test_start = time.process_time()\n",
    "    test_pred = classifier.predict(test_x)\n",
    "    test_end = time.process_time()\n",
    "    test_time = test_end - test_start\n",
    "    acc_macro = class_accuracy_score(test_y, test_pred, average=\"macro\")\n",
    "    acc_micro = class_accuracy_score(test_y, test_pred, average=\"micro\")\n",
    "    pre_macro = precision_score(test_y, test_pred, average='macro', zero_division=1)\n",
    "    pre_micro = precision_score(test_y, test_pred, average='micro', zero_division=1)\n",
    "    rec_macro = recall_score(test_y, test_pred, average='macro', zero_division=1)\n",
    "    rec_micro = recall_score(test_y, test_pred, average='micro', zero_division=1)\n",
    "    benchmark_results[name] = (name, acc_macro, acc_micro, pre_macro, pre_micro, rec_macro, rec_micro, train_time, test_time)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd88cde6",
   "metadata": {},
   "source": [
    "1. Naïve Bayes(Bernoulli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "7b26ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "benchmark_single_class(bnb, 'Naïve Bayes(Bernoulli)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8373d55c",
   "metadata": {},
   "source": [
    "2. Naïve Bayes(Multinomial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "2955176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "benchmark_single_class(mnb, 'Naïve Bayes(Multinomial)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fb1b84c",
   "metadata": {},
   "source": [
    "3. KNN(k=3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "9b56f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn3 = KNeighborsClassifier(3)\n",
    "benchmark_single_class(knn3, 'KNN(k=3)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33358d72",
   "metadata": {},
   "source": [
    "4. KNN(k=4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "3ab92f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn4 = KNeighborsClassifier(4)\n",
    "benchmark_single_class(knn4, 'KNN(k=4)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57ec1201",
   "metadata": {},
   "source": [
    "5. KNN(k=5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "1238e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn5 = KNeighborsClassifier(5)\n",
    "benchmark_single_class(knn5, 'KNN(k=5)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ece4473",
   "metadata": {},
   "source": [
    "6. SVM(Gaussian kernel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d87d5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svmg = SVC(kernel='rbf')\n",
    "# benchmark_single_class(svmg, 'SVM(Gaussian kernel)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8954babb",
   "metadata": {},
   "source": [
    "7. SVM(Linear kernel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "7dd36928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svml = SVC(kernel='linear')\n",
    "# benchmark_single_class(svml, 'SVM(Linear kernel)[libsvm]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "921ab1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svml2 = LinearSVC()\n",
    "benchmark_single_class(svml2, 'SVM(Linear kernel)[liblinear]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d14dea4c",
   "metadata": {},
   "source": [
    "### Evaluation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "38c83062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy macro</th>\n",
       "      <th>accuracy micro</th>\n",
       "      <th>precision macro</th>\n",
       "      <th>precision micro</th>\n",
       "      <th>recall macro</th>\n",
       "      <th>recall micro</th>\n",
       "      <th>time train</th>\n",
       "      <th>time test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naïve Bayes(Bernoulli)</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.553538</td>\n",
       "      <td>0.764714</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.553538</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naïve Bayes(Multinomial)</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.562244</td>\n",
       "      <td>0.716064</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.562244</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN(k=3)</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.636994</td>\n",
       "      <td>0.634042</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.636994</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN(k=4)</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.662911</td>\n",
       "      <td>0.669021</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.662911</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN(k=5)</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.694293</td>\n",
       "      <td>0.717931</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.694293</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>4.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM(Linear kernel)[liblinear]</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.818435</td>\n",
       "      <td>0.846587</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.818435</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name  accuracy macro  accuracy micro  \\\n",
       "0         Naïve Bayes(Bernoulli)           0.652        0.553538   \n",
       "1       Naïve Bayes(Multinomial)           0.608        0.562244   \n",
       "2                       KNN(k=3)           0.622        0.636994   \n",
       "3                       KNN(k=4)           0.650        0.662911   \n",
       "4                       KNN(k=5)           0.688        0.694293   \n",
       "5  SVM(Linear kernel)[liblinear]           0.818        0.818435   \n",
       "\n",
       "   precision macro  precision micro  recall macro  recall micro  time train  \\\n",
       "0         0.764714            0.652      0.553538         0.652    0.343750   \n",
       "1         0.716064            0.608      0.562244         0.608    0.156250   \n",
       "2         0.634042            0.622      0.636994         0.622    0.015625   \n",
       "3         0.669021            0.650      0.662911         0.650    0.000000   \n",
       "4         0.717931            0.688      0.694293         0.688    0.234375   \n",
       "5         0.846587            0.818      0.818435         0.818    0.109375   \n",
       "\n",
       "   time test  \n",
       "0   0.046875  \n",
       "1   0.000000  \n",
       "2   4.921875  \n",
       "3   5.250000  \n",
       "4   4.921875  \n",
       "5   0.015625  "
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sc_df = pd.DataFrame(data=list(benchmark_results.values()),\n",
    "                     columns=['name', 'accuracy macro', 'accuracy micro', 'precision macro', 'precision micro', 'recall macro', 'recall micro', 'time train', 'time test'])\n",
    "\n",
    "sc_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58072d39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "35fd45f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvm = LinearSVC()\n",
    "lsvm.fit(train_x, train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb83ee6e",
   "metadata": {},
   "source": [
    ". Group data by parent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a119f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = {\n",
    "    'ABBR' : [],\n",
    "    'DESC' : [],\n",
    "    'ENTY' : [],\n",
    "    'HUM' : [],\n",
    "    'LOC' : [],\n",
    "    'NUM' : [],\n",
    "}\n",
    "\n",
    "for row in train_data:\n",
    "    sub_data[row.parent_class].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "69b1a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "sub_classifiers = {\n",
    "    'ABBR' : LinearSVC(),\n",
    "    'DESC' : LinearSVC(),\n",
    "    'ENTY' : LinearSVC(),\n",
    "    'HUM' : LinearSVC(),\n",
    "    'LOC' : LinearSVC(),\n",
    "    'NUM' : LinearSVC(),\n",
    "}\n",
    "\n",
    "for parent_class, classifier in sub_classifiers.items():\n",
    "    rows = sub_data[parent_class]\n",
    "    data = make_2d(np.array([q.vector.toarray() for q in rows]))\n",
    "    labels = [q.child_class for q in rows]\n",
    "    classifier.fit(data, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8452ed3a",
   "metadata": {},
   "source": [
    ". Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "1723eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       1.00      0.78      0.88         9\n",
      "        DESC       0.80      0.86      0.83       138\n",
      "        ENTY       0.71      0.67      0.69        94\n",
      "         HUM       0.76      0.95      0.84        65\n",
      "         LOC       0.87      0.88      0.87        81\n",
      "         NUM       0.95      0.77      0.85       113\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.85      0.82      0.83       500\n",
      "weighted avg       0.83      0.82      0.82       500\n",
      "\n",
      "accuracy: {'NUM': 0.7699115044247787, 'ABBR': 0.7777777777777778, 'HUM': 0.9538461538461539, 'LOC': 0.8765432098765432, 'ENTY': 0.6702127659574468, 'DESC': 0.8623188405797102}\n"
     ]
    }
   ],
   "source": [
    "predicted_parent_classes = lsvm.predict(test_x)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, predicted_parent_classes))\n",
    "print(f'accuracy: ' + str(class_accuracy_score(test_y, predicted_parent_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d80cc25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_x = {\n",
    "    'ABBR' : [],\n",
    "    'DESC' : [],\n",
    "    'ENTY' : [],\n",
    "    'HUM' : [],\n",
    "    'LOC' : [],\n",
    "    'NUM' : [],\n",
    "}\n",
    "\n",
    "sub_test_y = {\n",
    "    'ABBR' : [],\n",
    "    'DESC' : [],\n",
    "    'ENTY' : [],\n",
    "    'HUM' : [],\n",
    "    'LOC' : [],\n",
    "    'NUM' : [],\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    vector = test_x[i]\n",
    "    sub_class = test_data[i].child_class\n",
    "    predicted_parent_class = predicted_parent_classes[i]\n",
    "    sub_test_x[predicted_parent_class].append(vector)\n",
    "    sub_test_y[predicted_parent_class].append(sub_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "82d169ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abb       1.00      0.00      0.00         1\n",
      "         exp       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.86         7\n",
      "   macro avg       0.93      0.50      0.46         7\n",
      "weighted avg       0.88      0.86      0.79         7\n",
      "\n",
      "accuracy: {'abb': 0.0, 'exp': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      animal       1.00      0.00      0.00         2\n",
      "        date       1.00      0.00      0.00         3\n",
      "         def       0.87      0.99      0.92       110\n",
      "        desc       0.29      0.40      0.33         5\n",
      "        dist       1.00      0.00      0.00         1\n",
      "         exp       1.00      0.00      0.00         2\n",
      "        food       1.00      0.00      0.00         1\n",
      "         ind       1.00      0.00      0.00         1\n",
      "      manner       0.40      1.00      0.57         2\n",
      "       other       1.00      0.00      0.00         6\n",
      "     product       1.00      0.00      0.00         3\n",
      "      reason       0.20      1.00      0.33         2\n",
      "       speed       1.00      0.00      0.00         1\n",
      "       state       1.00      0.00      0.00         2\n",
      "   substance       1.00      0.00      0.00         6\n",
      "        temp       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.78       148\n",
      "   macro avg       0.86      0.21      0.14       148\n",
      "weighted avg       0.86      0.78      0.71       148\n",
      "\n",
      "accuracy: {'def': 0.990909090909091, 'date': 0.0, 'desc': 0.4, 'exp': 0.0, 'dist': 0.0, 'temp': 0.0, 'reason': 1.0, 'speed': 0.0, 'other': 0.0, 'animal': 0.0, 'substance': 0.0, 'state': 0.0, 'food': 0.0, 'manner': 1.0, 'ind': 0.0, 'product': 0.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      animal       0.90      0.75      0.82        12\n",
      "        body       1.00      1.00      1.00         1\n",
      "       color       0.91      1.00      0.95        10\n",
      "      cremat       0.00      1.00      0.00         0\n",
      "    currency       0.86      1.00      0.92         6\n",
      "        date       1.00      0.00      0.00         1\n",
      "         def       1.00      0.00      0.00        12\n",
      "        desc       1.00      0.00      0.00         1\n",
      "      dismed       0.33      1.00      0.50         1\n",
      "        dist       1.00      0.00      0.00         3\n",
      "       event       0.50      1.00      0.67         1\n",
      "        food       0.33      0.67      0.44         3\n",
      "         ind       1.00      0.00      0.00         2\n",
      "      instru       1.00      1.00      1.00         1\n",
      "        lang       0.67      1.00      0.80         2\n",
      "       other       0.24      0.71      0.36         7\n",
      "       plant       1.00      1.00      1.00         1\n",
      "     product       1.00      1.00      1.00         1\n",
      "      reason       1.00      0.00      0.00         3\n",
      "       sport       1.00      1.00      1.00         1\n",
      "       state       1.00      0.00      0.00         1\n",
      "   substance       0.43      0.38      0.40         8\n",
      "    techmeth       1.00      1.00      1.00         1\n",
      "      termeq       0.44      0.67      0.53         6\n",
      "         veh       1.00      0.50      0.67         4\n",
      "        word       0.00      1.00      0.00         0\n",
      "\n",
      "    accuracy                           0.57        89\n",
      "   macro avg       0.75      0.64      0.50        89\n",
      "weighted avg       0.77      0.57      0.52        89\n",
      "\n",
      "accuracy: {'lang': 1.0, 'date': 0.0, 'currency': 1.0, 'instru': 1.0, 'animal': 0.75, 'state': 0.0, 'food': 0.6666666666666666, 'def': 0.0, 'desc': 0.0, 'dist': 0.0, 'reason': 0.0, 'sport': 1.0, 'substance': 0.375, 'techmeth': 1.0, 'termeq': 0.6666666666666666, 'other': 0.7142857142857143, 'event': 1.0, 'plant': 1.0, 'ind': 0.0, 'body': 1.0, 'dismed': 1.0, 'color': 1.0, 'veh': 0.5, 'product': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      animal       1.00      0.00      0.00         2\n",
      "        body       1.00      0.00      0.00         1\n",
      "        date       1.00      0.00      0.00         6\n",
      "        desc       1.00      0.25      0.40         4\n",
      "      dismed       1.00      0.00      0.00         1\n",
      "        dist       1.00      0.00      0.00         1\n",
      "       event       1.00      0.00      0.00         1\n",
      "          gr       0.50      0.67      0.57         6\n",
      "         ind       0.71      1.00      0.83        52\n",
      "       other       1.00      0.00      0.00         6\n",
      "      termeq       1.00      0.00      0.00         1\n",
      "       title       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.70        82\n",
      "   macro avg       0.93      0.16      0.15        82\n",
      "weighted avg       0.78      0.70      0.59        82\n",
      "\n",
      "accuracy: {'termeq': 0.0, 'date': 0.0, 'dismed': 0.0, 'desc': 0.25, 'body': 0.0, 'dist': 0.0, 'other': 0.0, 'animal': 0.0, 'gr': 0.6666666666666666, 'event': 0.0, 'title': 0.0, 'ind': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        city       1.00      0.83      0.91        18\n",
      "     country       0.60      1.00      0.75         3\n",
      "        date       1.00      0.00      0.00         3\n",
      "        dist       1.00      0.00      0.00         3\n",
      "       mount       1.00      1.00      1.00         3\n",
      "       other       0.88      0.98      0.92        44\n",
      "       plant       1.00      0.00      0.00         4\n",
      "       state       0.30      1.00      0.46         3\n",
      "   substance       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.82        82\n",
      "   macro avg       0.86      0.53      0.45        82\n",
      "weighted avg       0.89      0.82      0.78        82\n",
      "\n",
      "accuracy: {'date': 0.0, 'other': 0.9772727272727273, 'plant': 0.0, 'state': 1.0, 'mount': 1.0, 'dist': 0.0, 'city': 0.8333333333333334, 'substance': 0.0, 'country': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       count       0.82      1.00      0.90         9\n",
      "        date       0.89      1.00      0.94        34\n",
      "         def       1.00      0.00      0.00         1\n",
      "        dist       1.00      0.88      0.93         8\n",
      "       money       1.00      0.67      0.80         3\n",
      "       other       1.00      0.55      0.71        11\n",
      "        perc       0.60      1.00      0.75         3\n",
      "      period       0.67      1.00      0.80         8\n",
      "      reason       1.00      0.00      0.00         1\n",
      "       speed       1.00      0.80      0.89         5\n",
      "       state       1.00      0.00      0.00         1\n",
      "        temp       1.00      1.00      1.00         4\n",
      "      weight       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.87        92\n",
      "   macro avg       0.92      0.66      0.66        92\n",
      "weighted avg       0.90      0.87      0.85        92\n",
      "\n",
      "accuracy: {'def': 0.0, 'date': 1.0, 'dist': 0.875, 'temp': 1.0, 'count': 1.0, 'speed': 0.8, 'other': 0.5454545454545454, 'state': 0.0, 'money': 0.6666666666666666, 'weight': 0.75, 'period': 1.0, 'perc': 1.0, 'reason': 0.0}\n"
     ]
    }
   ],
   "source": [
    "sub_pred_test_y = {}\n",
    "\n",
    "for parent_class, queries in sub_test_x.items():\n",
    "    classifier = sub_classifiers[parent_class]\n",
    "    sub_pred_test_y[parent_class] = classifier.predict(queries)\n",
    "\n",
    "for parent_class, pred_y in sub_pred_test_y.items():\n",
    "    print(classification_report(sub_test_y[parent_class], pred_y, zero_division=1))\n",
    "    print(f'accuracy: ' + str(class_accuracy_score(sub_test_y[parent_class], pred_y)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02c5fff5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Clustering using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "64e1f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_train_rows = sub_data['LOC']\n",
    "location_train_x = make_2d(np.array([q.vector.toarray() for q in location_train_rows]))\n",
    "location_train_y = [q.child_class for q in location_train_rows]\n",
    "\n",
    "location_test_x = sub_test_x['LOC']\n",
    "location_test_y = sub_test_y['LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "a0798d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3: recall=0.0, precision=0.0, rand index=0.5338753387533876, silhouette=0.05132416569241707\n",
      "k=4: recall=0.0, precision=0.0, rand index=0.618187292984041, silhouette=0.062125107274616474\n",
      "k=5: recall=0.0, precision=0.0, rand index=0.695874736525143, silhouette=0.07470047495519952\n",
      "k=6: recall=0.0, precision=0.0, rand index=0.6847335140018067, silhouette=0.0760485363012754\n",
      "k=7: recall=0.0, precision=0.0, rand index=0.7305028605841614, silhouette=0.1129662942919057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, recall_score, precision_score, rand_score\n",
    "\n",
    "for n_clusters in range(3, 8):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=100)\n",
    "    kmeans.fit(location_train_x, location_train_y)\n",
    "    location_pred_y = kmeans.predict(location_test_x)\n",
    "    recall = 0.0#recall_score(location_test_y, location_pred_y)\n",
    "    precision = 0.0#precision_score(location_test_y, location_pred_y)\n",
    "    rand_index = rand_score(location_test_y, location_pred_y)\n",
    "    silhouette = silhouette_score(location_test_x, location_pred_y)\n",
    "    print(f'k={n_clusters}: recall={recall}, precision={precision}, rand index={rand_index}, silhouette={silhouette}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e0b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4ea2e4b26b0fd95219da9fe00dd3bb328c5d875c7deaef4b029faf977d454da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
